{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "452e04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy, nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28104b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Marcell/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ===== SETUP =====\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# load spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])  # POS & sents cukup\n",
    "\n",
    "# Stopwords + pengecualian (dipertahankan)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "exceptions = {\"your\", \"own\", \"how\", \"you\"}\n",
    "stop_words.difference_update(exceptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f6b6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== BT VERB LIST =====\n",
    "bt_verb_list = {\n",
    "    \"Knowledge\": [\"define\",\"duplicate\",\"list\",\"memorize\",\"recall\",\"repeat\",\"state\",\"identify\",\"recognize\"],\n",
    "    \"Comprehension\": [\"classify\",\"describe\",\"discuss\",\"explain\",\"identify\",\"locate\",\"recognize\",\"report\",\"select\",\"translate\",\"summarize\",\"interpret\"],\n",
    "    \"Application\": [\"apply\",\"choose\",\"demonstrate\",\"illustrate\",\"interpret\",\"operate\",\"schedule\",\"sketch\",\"solve\",\"use\",\"implement\"],\n",
    "    \"Analysis\": [\"analyze\",\"compare\",\"contrast\",\"differentiate\",\"discriminate\",\"distinguish\",\"examine\",\"experiment\",\"question\",\"test\",\"investigate\"],\n",
    "    \"Evaluation\": [\"appraise\",\"argue\",\"assess\",\"choose\",\"defend\",\"estimate\",\"evaluate\",\"judge\",\"justify\",\"rate\",\"support\",\"value\",\"critique\"],\n",
    "    \"Synthesis\": [\"assemble\",\"construct\",\"create\",\"design\",\"develop\",\"formulate\",\"write\",\"invent\",\"generate\",\"plan\",\"produce\",\"compose\"]\n",
    "}\n",
    "ALL_BT = set(v for vv in bt_verb_list.values() for v in vv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2860c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LOAD DATASET =====\n",
    "df = pd.read_csv(\"yahya_et_al_dataset.csv\")\n",
    "df = df.dropna(subset=[\"soal\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a9a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PIPELINE =====\n",
    "def preprocess_pipeline(text: str, log=False):\n",
    "    logs = []\n",
    "\n",
    "    # (1) Cleaning + (2) Lowercase\n",
    "    clean = re.sub(r\"[^A-Za-z\\s]\", \" \", str(text))\n",
    "    lower = clean.lower()\n",
    "\n",
    "    # (3) Tokenization (spaCy)  &  (4) POS Tagging\n",
    "    doc = nlp(lower)\n",
    "    if log:\n",
    "        tokens_raw = [t.text for t in doc if t.is_alpha]\n",
    "        logs.append(f\"[tokenize] -> {tokens_raw}\")\n",
    "        logs.append(f\"[pos] -> {[(t.text, t.pos_) for t in doc if t.is_alpha]}\")\n",
    "\n",
    "    # (5) Identification of BT Keyword (berbasis POSISI – sesuai paper)\n",
    "    bt_flags = {}  # index token spaCy -> True/False\n",
    "    for sent in doc.sents:\n",
    "        for i, tok in enumerate(sent):\n",
    "            if not tok.is_alpha:\n",
    "                continue\n",
    "            lemma = tok.lemma_.lower()\n",
    "            prev_tok = sent[i-1] if i > 0 else None\n",
    "\n",
    "            is_candidate = (\n",
    "                i == 0 or\n",
    "                (prev_tok is not None and (\n",
    "                    prev_tok.lower_ == \"and\" or\n",
    "                    (prev_tok.pos_ == \"ADV\" and prev_tok.text.endswith(\"ly\"))\n",
    "                ))\n",
    "            )\n",
    "            bt_flags[tok.i] = bool(is_candidate and (lemma in ALL_BT))\n",
    "\n",
    "    if log:\n",
    "        logs.append(f\"[bt-ident] -> {[(t.text, bt_flags.get(t.i, False)) for t in doc if t.is_alpha]}\")\n",
    "\n",
    "    # (6) Stop Word Removal (setelah identifikasi, agar posisi BT tidak terganggu)\n",
    "    kept = [t for t in doc if t.is_alpha and t.text not in stop_words]\n",
    "    if log:\n",
    "        logs.append(f\"[stopwords] -> {[t.text for t in kept]} (exceptions={sorted(exceptions)})\")\n",
    "\n",
    "    # (7) Lemmatization (pada token yang dipertahankan)\n",
    "    lemmas = [t.lemma_.lower() for t in kept]\n",
    "    if log:\n",
    "        logs.append(f\"[lemmatize] -> {lemmas}\")\n",
    "\n",
    "    # — Feature set (Unigram) + Proposed Weighting (ETFPOS bagian POS) —\n",
    "    weighted = []\n",
    "    detailed = []\n",
    "    for t in kept:\n",
    "        lemma = t.lemma_.lower()\n",
    "        is_bt = bt_flags.get(t.i, False)\n",
    "\n",
    "        if t.pos_ == \"VERB\":\n",
    "            w = 5.0 if is_bt else 3.0\n",
    "        elif t.pos_ in (\"NOUN\", \"PROPN\", \"ADJ\"):\n",
    "            w = 2.0\n",
    "        else:\n",
    "            w = 1.0\n",
    "\n",
    "        weighted.append((lemma, w))\n",
    "        if log:\n",
    "            detailed.append((t.text, t.pos_, \"BT\" if is_bt else \"non-BT\", w))\n",
    "\n",
    "    if log:\n",
    "        logs.append(f\"[pos-weight] -> {detailed}\")\n",
    "    return weighted, lemmas, logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09847aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PROSES SEMUA DOKUMEN =====\n",
    "weighted_docs = []\n",
    "lemma_docs = []\n",
    "for s in df[\"soal\"]:\n",
    "    w, lem, _ = preprocess_pipeline(s, log=False)\n",
    "    weighted_docs.append(w)\n",
    "    lemma_docs.append(lem)\n",
    "\n",
    "df[\"weighted_tokens\"] = weighted_docs\n",
    "df[\"lemmas\"] = lemma_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbae6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== IDF (berdasarkan unigram lemmas) =====\n",
    "vocab = sorted(set(tok for doc in df[\"lemmas\"] for tok in doc))\n",
    "idx = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "def compute_idf(list_of_docs):\n",
    "    N = len(list_of_docs)\n",
    "    df_count = {}\n",
    "    for doc in list_of_docs:\n",
    "        for w in set(doc):\n",
    "            df_count[w] = df_count.get(w, 0) + 1\n",
    "    # 1 + log((N+1)/(df+1)) stabil\n",
    "    return {w: 1.0 + np.log((N + 1) / (df_count[w] + 1)) for w in df_count}\n",
    "\n",
    "idf = compute_idf(df[\"lemmas\"].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06889e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ETFPOS-IDF vektor per dokumen =====\n",
    "def etfposidf_vector(weighted_tokens):\n",
    "    # ETFPOS(t,q) ~ (c(t)*w_pos) / sum(c*w_pos)\n",
    "    counts = {}\n",
    "    total_w = 0.0\n",
    "    for term, w in weighted_tokens:\n",
    "        counts[term] = counts.get(term, 0.0) + w\n",
    "        total_w += w\n",
    "\n",
    "    vec = np.zeros(len(vocab), dtype=float)\n",
    "    if total_w == 0:\n",
    "        return vec\n",
    "\n",
    "    for term, cw in counts.items():\n",
    "        j = idx.get(term)\n",
    "        if j is None:\n",
    "            continue\n",
    "        tfpos = cw / total_w                  # ETFPOS(t,q)\n",
    "        vec[j] = tfpos * idf.get(term, 1.0)   # × IDF → ETFPOS-IDF\n",
    "    return vec\n",
    "\n",
    "df[\"vec\"] = [etfposidf_vector(w) for w in df[\"weighted_tokens\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f8a058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MATRIX + L2 NORMALIZATION (sesuai rumus paper) =====\n",
    "X = np.vstack(df[\"vec\"].values) if len(df) else np.zeros((0, len(vocab)))\n",
    "X_norm = normalize(X, norm=\"l2\") if X.shape[0] > 0 else X  # Normalized ETFPOS-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5440c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selesai membentuk fitur ETFPOS-IDF.\n",
      "File: yahya_etfposidf.csv\n",
      "Jumlah dokumen (baris): 600\n",
      "Jumlah fitur (unigram): 1436\n",
      "Total kolom (termasuk meta): 1438\n"
     ]
    }
   ],
   "source": [
    "# ===== SIMPAN =====\n",
    "feat_df = pd.DataFrame(X_norm, columns=vocab)\n",
    "meta_cols = [c for c in [\"soal\", \"label\"] if c in df.columns]\n",
    "final_df = pd.concat([df[meta_cols].reset_index(drop=True), feat_df], axis=1)\n",
    "\n",
    "out_path = \"yahya_etfposidf.csv\"\n",
    "final_df.to_csv(out_path, index=False)\n",
    "\n",
    "# ===== RINGKASAN =====\n",
    "print(\"Selesai membentuk fitur ETFPOS-IDF.\")\n",
    "print(f\"File: {out_path}\")\n",
    "print(f\"Jumlah dokumen (baris): {final_df.shape[0]}\")\n",
    "print(f\"Jumlah fitur (unigram): {len(vocab)}\")\n",
    "print(f\"Total kolom (termasuk meta): {final_df.shape[1]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

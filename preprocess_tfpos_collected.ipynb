{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e19c3851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk, spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cf0276b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Marcell/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# === SETUP ===\n",
    "nltk.download(\"stopwords\")\n",
    "ps = PorterStemmer()\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])  # POS saja cukup\n",
    "\n",
    "# Stopwords dengan pengecualian\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "exceptions = {\"your\", \"own\", \"how\", \"you\"}\n",
    "stop_words.difference_update(exceptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e76b73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soal</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>List two reference parameters in the setHour f...</td>\n",
       "      <td>Knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Explain briefly the meaning of the following ...</td>\n",
       "      <td>Knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Label the parts of the diagram</td>\n",
       "      <td>Knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Based on the above dataType class, list all t...</td>\n",
       "      <td>Knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Define morphology</td>\n",
       "      <td>Knowledge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                soal      label\n",
       "0  List two reference parameters in the setHour f...  Knowledge\n",
       "1  \"Explain briefly the meaning of the following ...  Knowledge\n",
       "2                     Label the parts of the diagram  Knowledge\n",
       "3  \"Based on the above dataType class, list all t...  Knowledge\n",
       "4                                  Define morphology  Knowledge"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === LOAD DATASET ===\n",
    "df = pd.read_csv(\"collected_dataset.csv\")  # kolom: 'soal' (+ 'label' opsional)\n",
    "df = df.dropna(subset=[\"soal\"]).reset_index(drop=True)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c17bb55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soal</th>\n",
       "      <th>stems</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>List two reference parameters in the setHour f...</td>\n",
       "      <td>[list, two, refer, paramet, sethour, function]</td>\n",
       "      <td>[VERB, NUM, NOUN, NOUN, ADJ, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Explain briefly the meaning of the following ...</td>\n",
       "      <td>[explain, briefli, mean, follow, term, demogra...</td>\n",
       "      <td>[VERB, ADV, NOUN, VERB, NOUN, NOUN, NOUN, NOUN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Label the parts of the diagram</td>\n",
       "      <td>[label, part, diagram]</td>\n",
       "      <td>[PROPN, NOUN, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Based on the above dataType class, list all t...</td>\n",
       "      <td>[base, datatyp, class, list, function, member]</td>\n",
       "      <td>[VERB, NOUN, NOUN, NOUN, NOUN, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Define morphology</td>\n",
       "      <td>[defin, morpholog]</td>\n",
       "      <td>[VERB, NOUN]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                soal  \\\n",
       "0  List two reference parameters in the setHour f...   \n",
       "1  \"Explain briefly the meaning of the following ...   \n",
       "2                     Label the parts of the diagram   \n",
       "3  \"Based on the above dataType class, list all t...   \n",
       "4                                  Define morphology   \n",
       "\n",
       "                                               stems  \\\n",
       "0     [list, two, refer, paramet, sethour, function]   \n",
       "1  [explain, briefli, mean, follow, term, demogra...   \n",
       "2                             [label, part, diagram]   \n",
       "3     [base, datatyp, class, list, function, member]   \n",
       "4                                 [defin, morpholog]   \n",
       "\n",
       "                                            pos_tags  \n",
       "0                 [VERB, NUM, NOUN, NOUN, ADJ, NOUN]  \n",
       "1  [VERB, ADV, NOUN, VERB, NOUN, NOUN, NOUN, NOUN...  \n",
       "2                                [PROPN, NOUN, NOUN]  \n",
       "3               [VERB, NOUN, NOUN, NOUN, NOUN, NOUN]  \n",
       "4                                       [VERB, NOUN]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === PREPROCESS ===\n",
    "def preprocess_for_tfpos(text: str):\n",
    "    # Normalization + lowercase\n",
    "    clean = re.sub(r\"[^A-Za-z\\s]\", \" \", str(text)).lower()\n",
    "    # Stopword removal selektif + buang non-alpha\n",
    "    rough_tokens = [w for w in clean.split() if w.isalpha() and w not in stop_words]\n",
    "    # POS tagging\n",
    "    doc = nlp(\" \".join(rough_tokens))\n",
    "    # Stemming + POS\n",
    "    stems, pos_tags = [], []\n",
    "    for t in doc:\n",
    "        if not t.is_alpha:\n",
    "            continue\n",
    "        stems.append(ps.stem(t.text))\n",
    "        pos_tags.append(t.pos_)   # VERB, NOUN, ADJ, PROPN, dll.\n",
    "    return stems, pos_tags\n",
    "\n",
    "processed = df[\"soal\"].apply(preprocess_for_tfpos)\n",
    "df[\"stems\"]    = processed.apply(lambda x: x[0])\n",
    "df[\"pos_tags\"] = processed.apply(lambda x: x[1])\n",
    "\n",
    "df[[\"soal\", \"stems\", \"pos_tags\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5db40f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390,\n",
       " ['abstract',\n",
       "  'accept',\n",
       "  'accord',\n",
       "  'action',\n",
       "  'add',\n",
       "  'address',\n",
       "  'adopt',\n",
       "  'advertis',\n",
       "  'advic',\n",
       "  'advis',\n",
       "  'affect',\n",
       "  'air',\n",
       "  'along',\n",
       "  'alpha',\n",
       "  'altern',\n",
       "  'analysi',\n",
       "  'analyz',\n",
       "  'anim',\n",
       "  'anoth',\n",
       "  'answer'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === BANGUN VOCABULARY (berbasis STEM) ===\n",
    "vocab = sorted(set(w for doc in df[\"stems\"] for w in doc))\n",
    "idx = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "len(vocab), vocab[:20]  # cek jumlah dan contoh vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13050ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === IDF: 1 + log(D/df) (aman untuk df>0) ===\n",
    "def compute_idf(list_of_docs):\n",
    "    D = len(list_of_docs)\n",
    "    df_count = {}\n",
    "    for doc in list_of_docs:\n",
    "        for w in set(doc):\n",
    "            df_count[w] = df_count.get(w, 0) + 1\n",
    "    return {\n",
    "        w: 1.0 + np.log(D / df_count[w]) if df_count[w] > 0 else 1.0\n",
    "        for w in df_count\n",
    "    }\n",
    "\n",
    "idf = compute_idf(df[\"stems\"].tolist())\n",
    "\n",
    "len(idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fbdd7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "Name: tfposidf_vec, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === TFPOS-IDF ===\n",
    "def tfposidf_vector(stems, pos_tags):\n",
    "    # Bobot POS: VERB > NOUN/ADJ/PROPN > lainnya\n",
    "    w1, w2, w3 = 5.0, 3.0, 1.0\n",
    "    counts, total_w = {}, 0.0\n",
    "    for stem, pos in zip(stems, pos_tags):\n",
    "        if pos == \"VERB\":\n",
    "            wp = w1\n",
    "        elif pos in (\"NOUN\", \"ADJ\", \"PROPN\"):\n",
    "            wp = w2\n",
    "        else:\n",
    "            wp = w3\n",
    "        counts[stem] = counts.get(stem, 0.0) + wp\n",
    "        total_w += wp\n",
    "\n",
    "    vec = np.zeros(len(vocab), dtype=float)\n",
    "    if total_w == 0:\n",
    "        return vec\n",
    "\n",
    "    for stem, cw in counts.items():\n",
    "        j = idx.get(stem)\n",
    "        if j is None:\n",
    "            continue\n",
    "        tfpos = cw / total_w           # normalisasi dalam-dokumen\n",
    "        vec[j] = tfpos * idf.get(stem, 1.0)\n",
    "    return vec\n",
    "\n",
    "df[\"tfposidf_vec\"] = [\n",
    "    tfposidf_vector(stems, pos)\n",
    "    for stems, pos in zip(df[\"stems\"], df[\"pos_tags\"])\n",
    "]\n",
    "\n",
    "df[\"tfposidf_vec\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf1b6fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141, 390)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === MATRIX & L2 NORMALIZATION (opsional) ===\n",
    "X = np.vstack(df[\"tfposidf_vec\"].values) if len(df) > 0 else np.zeros((0, len(vocab)))\n",
    "X_norm = normalize(X, norm=\"l2\") if X.shape[0] > 0 else X  # matikan jika tak ingin L2\n",
    "\n",
    "X_norm.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8913405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SIMPAN 1: kolom fitur = KATA ===\n",
    "feat_words_df = pd.DataFrame(X_norm, columns=vocab)\n",
    "meta_cols = [c for c in [\"soal\", \"label\"] if c in df.columns]\n",
    "final_words_df = pd.concat([df[meta_cols].reset_index(drop=True), feat_words_df], axis=1)\n",
    "final_words_df.to_csv(\"collected_tfpos_words.csv\", index=False)\n",
    "\n",
    "# === SIMPAN 2: kolom fitur = dim_i ===\n",
    "feat_dims_df = pd.DataFrame(X_norm, columns=[f\"dim_{i}\" for i in range(len(vocab))])\n",
    "final_dims_df = pd.concat([df[meta_cols].reset_index(drop=True), feat_dims_df], axis=1)\n",
    "final_dims_df.to_csv(\"collected_tfpos_dims.csv\", index=False)\n",
    "\n",
    "with open(\"tfpos_vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for w in vocab:\n",
    "        f.write(w + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6972a119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selesai: TFPOS-IDF\n",
      "Dokumen (baris)         : 141\n",
      "Jumlah fitur (|vocab|)   : 390\n",
      "Output fitur = kata      : collected_tfpos_words.csv\n",
      "Output fitur = dim_i     : collected_tfpos_dims.csv\n",
      "Kamus dim_i -> kata      : tfpos_vocab.txt\n",
      "Total kolom (words ver.) : 392 (termasuk meta: ['soal', 'label'])\n",
      "Total kolom (dims ver.)  : 392 (termasuk meta: ['soal', 'label'])\n"
     ]
    }
   ],
   "source": [
    "# === INFORMASI HASIL ===\n",
    "print(\"\\nSelesai: TFPOS-IDF\")\n",
    "print(f\"Dokumen (baris)         : {final_words_df.shape[0]}\")\n",
    "print(f\"Jumlah fitur (|vocab|)   : {len(vocab)}\")\n",
    "print(f\"Output fitur = kata      : collected_tfpos_words.csv\")\n",
    "print(f\"Output fitur = dim_i     : collected_tfpos_dims.csv\")\n",
    "print(f\"Kamus dim_i -> kata      : tfpos_vocab.txt\")\n",
    "print(f\"Total kolom (words ver.) : {final_words_df.shape[1]} (termasuk meta: {meta_cols})\")\n",
    "print(f\"Total kolom (dims ver.)  : {final_dims_df.shape[1]} (termasuk meta: {meta_cols})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
